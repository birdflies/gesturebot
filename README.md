This repo contains the official implementation for the paper [A Framework for Integrating Gesture Generation Models into Interactive Conversational Agents](http://www.ifaamas.org/Proceedings/aamas2021/pdfs/p1779.pdf), published as a demonstration at the 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS),

by [Rajmund Nagy](https://nagyrajmund.github.io/), [Taras Kucherenko](https://svito-zar.github.io/), [Birger Moëll](https://www.kth.se/profile/bmoell?l=en), [André Pereira](https://sites.google.com/view/andre-pereira-phd), [Hedvig Kjellström](https://www.kth.se/profile/hedvig) and [Ulysses Bernardet](https://research.aston.ac.uk/en/persons/ulysses-bernardet).

-------------

We present a framework for integrating recent data-driven gesture generation models into interactive conversational agents in Unity. 
Our video demonstration is available below:

[![video demonstration](https://i.imgur.com/rqYRYam.png)](https://www.youtube.com/watch?v=jhgUBS0125A)


# Instructions for running the demo
This branch contains our DialogFlow version of our implementation, which requires a functioning [DialogFlow](https://cloud.google.com/dialogflow) project to run.
You may visit the [blenderbot_demo](https://github.com/nagyrajmund/gesturebot/) branch for an alternative version with a built-in Blenderbot chatbot.

Please follow the instructions in [INSTALLATION.MD](INSTALLATION.md) to install and run the project.

## Architecture

Our framework is designed to be fully modular therefore it can be applied to different voices, chatbot backends, gesture generation models and 3D characters. However, using it in a new project will require some coding for which we provide guidance below.

![](https://i.imgur.com/PSW6a23.jpg)

### Unity integration
The source code of the Unity scene with DialogFlow integration is available [on this link](https://drive.google.com/file/d/14URIJxO9vyMNHGWbkRyz_jEIiHPGhByM/view?usp=sharing), while the Blenderbot version is available [here](https://drive.google.com/file/d/1OTHe-0IaVKN2WRWusZlE9q259qOihXxj/view?usp=sharing). The relevant C# scripts are found in the `Assets/Scripts/` folder.

- The C# and the python scripts comunicate over ActiveMQ, as implemented in the `ActiveMQClient.cs` and the [`messaging_server.py`](gesticulator/gesticulator/interface/messaging_server.py) files.
- Once the generated motion arrives to the 3D agent, the `MotionVisualizer.cs` file animates its model by modifying the `localRotation` values of each joint.
  - There is no clear convention of how 3D models handle joint rotations in Unity. The 3D joint angles generated by Gesticulator follow the [BVH format](https://research.cs.wisc.edu/graphics/Courses/cs-838-1999/Jeff/BVH.html); applying them to new character models will require Unity knowledge and some tinkering.

### Chatbot backend
- In the [dialogflow_demo](https://github.com/nagyrajmund/gesturebot/dialogflow_demo/), the agent's responses are generated by DialogFlow, which is integrated in the C# script `DialogFlowCommunicator.cs`.
- In the [blenderbot_demo](https://github.com/nagyrajmund/gesturebot/), the responses are generated with Facebook's Blenderbot and Glow-TTS as implemented in [Mozilla's TTS library](https://github.com/mozilla/TTS). See [blenderbot.py](https://github.com/nagyrajmund/gesturebot/blob/blenderbot_demo/gesticulator/gesticulator/interface/blenderbot.py) and [tts_interface.py](https://github.com/nagyrajmund/gesturebot/blob/blenderbot_demo/gesticulator/gesticulator/interface/tts_interface.py) for details.

### Gesture generation
- We use the [Gesticulator](https://github.com/Svito-zar/gesticulator/) model in both demonstrations, which generates motion as 3D joint angles using speech text and audio as input.
- In order to use other models, the following have to be considered:
  - 3D joint angles are necessary to animate the 3D model in Unity, therefore the gesture generation model must return the motion in that format.
  - For any model, an interface must be implemented for getting the generated gestures for any speech. The [GesturePredictor class](https://github.com/nagyrajmund/gesturebot/blob/2423a99ef23c88bb3d9418f540c74421756a2bbf/gesticulator/gesticulator/interface/profiling/gesture_predictor.py#L19) shows how we implemented that for Gesticulator.
- [StyleGestures](https://github.com/simonalexanderson/StyleGestures/) is a good alternative model with a compatible codebase.

###  

# Acknowledgements
The authors would like to thank [Lewis King](https://lewisbenking.github.io/) for sharing the source code of his JimBot project with us.

# Citation
If you use this code in your research, then please cite it:

```
@inproceedings{Nagy2021gesturebot,
author = {Nagy, Rajmund and Kucherenko, Taras and Moell, Birger and Pereira, Andr\'{e} and Kjellstr\"{o}m, Hedvig and Bernardet, Ulysses},
title = {A Framework for Integrating Gesture Generation Models into Interactive Conversational Agents},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}
```
