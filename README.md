This repo contains the official implementation for the paper [A Framework for Integrating Gesture Generation Models into Interactive Conversational Agents](http://www.ifaamas.org/Proceedings/aamas2021/pdfs/p1779.pdf), published as a demonstration at the 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS),

by [Rajmund Nagy](https://nagyrajmund.github.io/), [Taras Kucherenko](https://svito-zar.github.io/), [Birger Moëll](https://www.kth.se/profile/bmoell?l=en), [André Pereira](https://sites.google.com/view/andre-pereira-phd), [Hedvig Kjellström](https://www.kth.se/profile/hedvig) and [Ulysses Bernardet](https://research.aston.ac.uk/en/persons/ulysses-bernardet).

-------------

We present a framework for integrating recent data-driven gesture generation models into interactive conversational agents in Unity. 
Our video demonstration is available below:

[![video demonstration](https://i.imgur.com/rqYRYam.png)](https://www.youtube.com/watch?v=jhgUBS0125A)


# Instructions for running the demo
This branch contains the Blenderbot version of our implementation with a built-in chatbot and TTS. You may visit the [dialogflow_demo](https://github.com/nagyrajmund/gesturebot/tree/dialogflow_demo/) branch for an alternative version that integrates [DialogFlow](https://cloud.google.com/dialogflow) to the project for speech generation.

Please follow the instructions in [INSTALLATION.md](INSTALLATION.md) to install and run the project.

## Architecture

Our framework is designed to be fully modular therefore it can be applied to different voices, chatbot backends, gesture generation models and 3D characters. However, using it in a new project will require some coding for which we provide guidance below.

![](https://i.imgur.com/PSW6a23.jpg)

### Unity integration
The source code of the Unity scene with DialogFlow integration is available [on this link](https://drive.google.com/file/d/14URIJxO9vyMNHGWbkRyz_jEIiHPGhByM/view?usp=sharing), while the Blenderbot version is available [here](https://drive.google.com/file/d/1OTHe-0IaVKN2WRWusZlE9q259qOihXxj/view?usp=sharing). The relevant C# scripts are found in the `Assets/Scripts/` folder. The entry point of the python code is the [`main.py`](gesticulator/gesticulator/interface/main.py) file, while the bulk of the implementation is found in [`gesture_generator_service.py`](gesticulator/gesticulator/interface/gesture_generator_service.py).

- The C# and the python scripts comunicate over ActiveMQ, as implemented in the `ActiveMQClient.cs` and the [`messaging_server.py`](gesticulator/gesticulator/interface/messaging_server.py) files.
- Once the generated motion arrives to the 3D agent, the `MotionVisualizer.cs` file animates its model by modifying the `localRotation` values of each joint.
  - There is no clear convention of how 3D models handle joint rotations in Unity. The 3D joint angles generated by Gesticulator follow the [BVH format](https://research.cs.wisc.edu/graphics/Courses/cs-838-1999/Jeff/BVH.html); applying them to new character models will require Unity knowledge and some tinkering.

### Chatbot backend
- In the [dialogflow_demo](https://github.com/nagyrajmund/gesturebot/tree/dialogflow_demo/), the agent's responses are generated by DialogFlow, which is integrated in the C# script `DialogFlowCommunicator.cs`.
- In the [blenderbot_demo](https://github.com/nagyrajmund/gesturebot/), the responses are generated with Facebook's Blenderbot and Glow-TTS as implemented in [Mozilla's TTS library](https://github.com/mozilla/TTS). See [`blenderbot.py`](https://github.com/nagyrajmund/gesturebot/blob/blenderbot_demo/gesticulator/gesticulator/interface/blenderbot.py) and [`tts_interface.py`](https://github.com/nagyrajmund/gesturebot/blob/blenderbot_demo/gesticulator/gesticulator/interface/tts_interface.py) for details.

### Gesture generation
- We use the [Gesticulator](https://github.com/Svito-zar/gesticulator/) model in both demonstrations, which generates motion as 3D joint angles using speech text and audio as input.
- In order to use other models, the following have to be considered:
  - 3D joint angles are necessary to animate the 3D model in Unity, therefore the gesture generation model must return the motion in that format.
  - For any model, an interface must be implemented for getting the generated gestures for any speech. The [`GesturePredictor` class](https://github.com/nagyrajmund/gesturebot/blob/0e359c4bb64a4eb2203146738d583c3b10137871/gesticulator/gesticulator/interface/profiling/gesture_predictor.py#L19) shows how we implemented that for Gesticulator.
- [StyleGestures](https://github.com/simonalexanderson/StyleGestures/) is a good alternative model with a compatible codebase.

###  Other 
The recommended version of Unity is preferably 2020.3.16f1. 
If there is no talk and video button in scenes of Unity. You can remove "Newtonsoft.Json.dll" file in Plugins subpath，it seems to be incompatible.

# Acknowledgements
The authors would like to thank [Lewis King](https://lewisbenking.github.io/) for sharing the source code of his JimBot project with us.

# Citation
If you use this code in your research, then please cite it:

```
@inproceedings{Nagy2021gesturebot,
author = {Nagy, Rajmund and Kucherenko, Taras and Moell, Birger and Pereira, Andr\'{e} and Kjellstr\"{o}m, Hedvig and Bernardet, Ulysses},
title = {A Framework for Integrating Gesture Generation Models into Interactive Conversational Agents},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}
```
